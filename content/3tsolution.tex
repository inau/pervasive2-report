\section{System Implementation} 
As can be deduced from the above overview, the system is quite complex,
this means that there has been some challenges when building the system.

Initially the concept was to have pure bluetooth communication between the 3 main components,
but due to unexpected challenges a fourth element was introduced to handle communication between android and the weka system. 
However this also means that the system is not limited by the short range of the bluetooth receiver.
The flow we ended up with is shown in figure \ref{fig:sys}.

\subsection{Android Image Presenter}
The android application has been designed to handle communication over a webservice in the background, 
while the presentation layer is easily exchangeable due to an interface as seen on figure \ref{fig:and_class}.


The flow of information is quite simple, when the GestureService has a non-empty result from the webservice,
it propagates this information to the GestureHandler, which has knowledge about any GestureListener instance.


The Mainactivity.class implements the GestureListener.
A custom ImageView has been designed to expose functions needed such as zoom and pan, which are manipulated via the aforementioned listener.

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\columnwidth]{img/android_class_diagram}
\caption{Android Application Class diagram.}
\label{fig:and_class}
\end{figure}


\subsection{Gesture Recognition}
\subsubsection{Preprocessing}
Once the acceleration and rotation data is sent from the device to the computer,
 the values are smoothed with an average of the 20 previous values in order to avoid and reduce the effect of noise on the sensors.
 This phase is called preprocessing of the data. 
 This allowed us to have more precise information, as can be deducted from the images  \ref{fig:figure2} and \ref{fig:figure3}

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\columnwidth]{img/raw}
\caption{Data from the device before preprocessing.}
\label{fig:figure2}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\columnwidth]{img/20}
\caption{Data from the device after preprocessing.}
\label{fig:figure3}
\end{figure}

\subsubsection{Evaluation}
For the actual processing and gesture recognition, we use a sliding window approach on the time series data we receive.
The window is of size 50, hence it will evaluate a sequence of 50 movement instances in a sequence,
every instance consisting of 6 values.
The window is able to capture a period of time of about 2 seconds, as the sampling rate of the device is about 25 Hz.
We organized known gestures in a very large training set, each individual gesture stored as a list of 50 * 6 values plus an identifier, in a way that every gesture will be compatible with the sliding window. 
The training data set in the final version of the project reached more than 500 entries in total.
We use Weka 3.6 to evaluate newly received data using a BayesNet classifier and comparing it to the training set. 
This resulted as the most accurate classifier for our model, giving up to 100\% accuracy when testing it.
The evaluation of the gesture is performed every 10 * 6 new values received.









